<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Neural ODE Blogpost</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#ok-so-what-are-neural-odes">Ok so what <em>are</em> neural ODEs?</a></li>
<li><a href="#how-the-neural-ode-model-computes-its-prediction">How the Neural ODE model computes its prediction:</a></li>
</ul>
</nav>
<p># Neural ODEs</p>
<p>I recently learned about neural ODEs and I found some things a bit confusing at first so I thought I might explain them from my perspective to possibly help others to better or easier understand the topic. That being said, there are other good resources to this material, I’ll link to them later in the post.</p>
<h3 id="ok-so-what-are-neural-odes">Ok so what <em>are</em> neural ODEs?</h3>
<p>Most importantly, they are <em>not</em> a new network architecture by themselves. I like to think of them as a framework <em>around</em> an existing neural network, changing the interpretation of that network and they way it is used.</p>
<p>I’ll assume some familiarity with multivariate calculus for this post.</p>
<p>Let’s say you have some system, where you don’t quite know how it evolves over time, but you can periodically measure it’s state. You decide that this system likely follows an ordinary differential equation (ODE). If we denote the state of the system at time <span class="math inline">\(t\)</span> with <span class="math inline">\(y(t)\)</span>, then an ODE of order <span class="math inline">\(n\)</span> is an equation of the form <span class="math inline">\(y^{(n)} = F(t, y^{(n-1)}, \ldots, y)\)</span>, i.e. the <span class="math inline">\(n-th\)</span> derivative of <span class="math inline">\(y\)</span> behaves according to some function <span class="math inline">\(F\)</span> of the lower order derivatives, and potentially explicitly of the time <span class="math inline">\(t\)</span>.</p>
<p>For the remainder of this post we will only look at first-order ODEs, i.e. equations of the form <span class="math inline">\(\dot y(t) = f(y(t), t)\)</span> for some function <span class="math inline">\(f\)</span>. This is not as restrictive as it looks since we can introduce auxiliary variables <span class="math inline">\(z(t) = \dot y(t)\)</span> to create a system of first-order ODEs from a second order ODE, and repeat the process for higher order ODEs [citation needed].</p>
<p>For a neural ODE you need two essential things:</p>
<ul>
<li>A neural network. This network should take as input a value <span class="math inline">\(y(t)\)</span> and a time <span class="math inline">\(t\)</span> and put out its estimate of <span class="math inline">\(f(y(t), t)\)</span>. Almost any commonly used neural network architecture is fair game. Some forms of attention mechanisms have problems around the uniqueness of the found solutions because they are not Lipschitz, but except for those, go nuts.</li>
<li>A (blackbox) ODE solver. This solver should take as input our function <span class="math inline">\(f\)</span> from above, the initial value <span class="math inline">\(y(t_0)\)</span> and an end time <span class="math inline">\(t^\star\)</span>. It then calculates (an approximation of) <span class="math inline">\(y(t^\star)\)</span> based on the given dynamics model <span class="math inline">\(f\)</span>. I won’t go into any detail of how ODE solvers work. Okay, now that we have those two building blocks, we can build our neural ODE model!</li>
</ul>
<h3 id="how-the-neural-ode-model-computes-its-prediction">How the Neural ODE model computes its prediction:</h3>
<ul>
<li>The input to the model is a tuple of the initial state <span class="math inline">\(y(t_0)\)</span>, the initial time <span class="math inline">\(t_0\)</span> and the end time <span class="math inline">\(t^\star\)</span>.</li>
<li>At step <span class="math inline">\(k\)</span> in the training process the neural network function is given as <span class="math inline">\(f_k(y, t)\)</span>.</li>
<li>For the given initial state, initial time and dynamics model (given by the neural network), the ODE solver now computes <span class="math inline">\(y(t^\star)\)</span>, i.e. the state of the system at time <span class="math inline">\(t^\star\)</span> if started in <span class="math inline">\(y(t_0)\)</span> at time <span class="math inline">\(t_0\)</span> and the system evolves under <span class="math inline">\(f_k(y, t)\)</span>.</li>
<li>This predicted <span class="math inline">\(y(t^\star)\)</span> can now be scored against our ground truth in the training set, e.g. via the MSE loss.</li>
</ul>
<p>Usually this is the point where we call the backward pass function and let the autodiff library of our choice do its magic. But… how the <em>hell</em> do we compute the gradients of the loss in this case? We have a blackbox ODE which calls our neural network many many times over the course of a single forward pass and its outputs at every step depend on all the previous outputs. Yes, we could do BPTT, but that would be very expensive and potentially suffer from similar issues as long-run dependencies in RNNs.</p>
<p>Luckily, there is a solution to more efficiently compute the gradient, using something called the adjoint method.</p>
</body>
</html>
